import os
import torch
import pandas as pd
import numpy as np
from PIL import Image
from tqdm.auto import tqdm
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler

# Metrics libraries (æŒ‡æ ‡è®¡ç®—åº“)
from torchmetrics.image import StructuralSimilarityIndexMeasure, PeakSignalNoiseRatio, LearnedPerceptualImagePatchSimilarity
from torchmetrics.image.fid import FrechetInceptionDistance
from torchvision.io import read_image

# ==========================================
# 1. Configuration (é…ç½®åŒºåŸŸ)
# ==========================================
# Path to your BEST checkpoint
# æ¨¡å‹è·¯å¾„ï¼šæ ¹æ®ä½ ä¹‹å‰çš„â€œè§†è§‰æµ‹è¯•â€ï¼Œå¦‚æœè§‰å¾— Epoch 8 æœ€å¥½ï¼Œå°±æŠŠä¸‹é¢çš„ 10 æ”¹æˆ 8
CHECKPOINT_PATH = "./sat2street_output/checkpoint-epoch-10" 

# Dataset paths
# æ•°æ®è·¯å¾„ (è‡ªåŠ¨åˆ¤æ–­ Notebook ä½ç½®)
if os.path.exists("pairs.csv"):
    DATA_ROOT = "." 
else:
    DATA_ROOT = "./data"

CSV_PATH = os.path.join(DATA_ROOT, "pairs.csv")
IMAGE_DIR = os.path.join(DATA_ROOT, "images")

# ğŸ”¥ Output directory (ç»“æœä¿å­˜åˆ° D ç›˜) ğŸ”¥
# ä½¿ç”¨ r"..." é˜²æ­¢è·¯å¾„è½¬ä¹‰é”™è¯¯
EVAL_ROOT = r"D:\yifan_2025\evaluation_results"

# Number of samples per category
# æ¯ä¸ªåˆ†ç±»æµ‹è¯•å¤šå°‘å¼ ï¼Ÿ(å»ºè®® 50 ä»¥ä¿è¯ FID æœ‰æ•ˆï¼Œæ˜¾å¡å¥½å¯ä»¥è®¾ä¸º None è·‘å…¨é‡)
SAMPLES_PER_CATEGORY = 50 

# ==========================================
# 2. Setup Metrics (åˆå§‹åŒ–æŒ‡æ ‡å·¥å…·)
# ==========================================
print("âš™ï¸ Initializing metrics calculators (åˆå§‹åŒ–è¯„ä¼°æŒ‡æ ‡)...")
device = "cuda" if torch.cuda.is_available() else "cpu"

# Initialize metrics on GPU to speed up
# åœ¨ GPU ä¸Šåˆå§‹åŒ– SSIM, PSNR, LPIPS, FID
ssim_calc = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)
psnr_calc = PeakSignalNoiseRatio(data_range=1.0).to(device)
lpips_calc = LearnedPerceptualImagePatchSimilarity(net_type='vgg').to(device)
# FID éœ€è¦è¾ƒå¤šæ˜¾å­˜ï¼Œå¦‚æœçˆ†æ˜¾å­˜å¯ä»¥å°† feature æ”¹ä¸º 64
fid_calc = FrechetInceptionDistance(feature=2048).to(device)

# ==========================================
# 3. Load Model (åŠ è½½æ¨¡å‹)
# ==========================================
print(f"ğŸš€ Loading model from: {CHECKPOINT_PATH}")
try:
    controlnet = ControlNetModel.from_pretrained(CHECKPOINT_PATH, torch_dtype=torch.float16)
    pipe = StableDiffusionControlNetPipeline.from_pretrained(
        "runwayml/stable-diffusion-v1-5", 
        controlnet=controlnet, 
        torch_dtype=torch.float16, 
        safety_checker=None
    ).to(device)
    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
    # CPU Offload saves VRAM for metric calculation
    # å¼€å¯ CPU å¸è½½ä»¥èŠ‚çœæ˜¾å­˜ç»™æŒ‡æ ‡è®¡ç®—ä½¿ç”¨
    pipe.enable_model_cpu_offload()
except Exception as e:
    print(f"âŒ Failed to load model. Error: {e}")
    exit()

# ==========================================
# 4. Evaluation Loop (ä¸»è¯„ä¼°å¾ªç¯)
# ==========================================
def evaluate_and_calculate():
    # Create results folder
    # åˆ›å»ºç»“æœæ–‡ä»¶å¤¹
    if not os.path.exists(EVAL_ROOT):
        os.makedirs(EVAL_ROOT, exist_ok=True)
    
    # Read CSV
    df = pd.read_csv(CSV_PATH)
    severities = df['severity'].unique()
    
    final_results = []

    # Iterate through each severity category
    # éå†æ¯ä¸€ä¸ªç¾å®³ç­‰çº§
    for severity in severities:
        print(f"\n{'='*40}")
        print(f"ğŸ§ Processing Category: {severity}")
        print(f"{'='*40}")
        
        # Create temp folders for FID calculation (Required by torch-fidelity)
        # ä¸º FID è®¡ç®—åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤¹ (å¿…é¡»ä¿å­˜æˆæ–‡ä»¶æ‰èƒ½ç®— FID)
        cat_gen_dir = os.path.join(EVAL_ROOT, severity, "generated")
        cat_gt_dir = os.path.join(EVAL_ROOT, severity, "ground_truth")
        os.makedirs(cat_gen_dir, exist_ok=True)
        os.makedirs(cat_gt_dir, exist_ok=True)

        # Filter data
        # ç­›é€‰æ•°æ®
        cat_df = df[df['severity'] == severity]
        if SAMPLES_PER_CATEGORY:
            n_sample = min(SAMPLES_PER_CATEGORY, len(cat_df))
            cat_df = cat_df.sample(n_sample, random_state=42)
        
        # Storage for scores
        scores = {"ssim": [], "psnr": [], "lpips": []}
        
        # Reset FID for this category
        # é‡ç½® FID è®¡ç®—å™¨
        fid_calc.reset()

        # Loop through samples
        # éå†æ ·æœ¬
        for idx, row in tqdm(cat_df.iterrows(), total=len(cat_df), desc="Generating & Measuring"):
            try:
                # 1. Load Images
                # è¯»å–å›¾ç‰‡
                sat_name = os.path.basename(row["sat_path"])
                svi_name = os.path.basename(row["svi_path"])
                
                sat_path = os.path.join(IMAGE_DIR, sat_name)
                gt_path = os.path.join(IMAGE_DIR, svi_name)
                
                if not os.path.exists(sat_path) or not os.path.exists(gt_path):
                    continue

                sat_img = Image.open(sat_path).convert("RGB")
                gt_img = Image.open(gt_path).convert("RGB")
                
                # 2. Get Original Dimensions
                # [å…³é”®] è·å–çœŸå®å›¾ç‰‡çš„åŸå§‹å°ºå¯¸ (ä¸ºäº†åé¢è¿˜åŸ)
                orig_w, orig_h = gt_img.size
                
                # 3. Generate
                # ç”Ÿæˆ (è¾“å…¥å…ˆç¼©æ”¾åˆ° 512x512 å–‚ç»™æ¨¡å‹)
                input_sat = sat_img.resize((512, 512), Image.BICUBIC)
                prompt = f"street view photography, realistic, ground level view, {str(severity).replace('_', ' ').lower()}, high quality, 4k"
                
                with torch.inference_mode():
                    gen_img = pipe(
                        prompt, 
                        image=input_sat, 
                        num_inference_steps=20,
                        controlnet_conditioning_scale=1.0
                    ).images[0]
                
                # 4. Resize back to Original Size
                # [å…³é”®] è¿˜åŸå°ºå¯¸ï¼šå¼ºåˆ¶æ‹‰ä¼¸å›åŸå§‹å¤§å°ï¼Œç¡®ä¿å’Œ GT ä¸€è‡´
                gen_img = gen_img.resize((orig_w, orig_h), Image.LANCZOS)
                
                # 5. Save to disk
                # ä¿å­˜å›¾ç‰‡ (FID è®¡ç®—éœ€è¦è¯»å–è¿™äº›æ–‡ä»¶)
                save_name = f"{idx}.png"
                gen_save_path = os.path.join(cat_gen_dir, save_name)
                gt_save_path = os.path.join(cat_gt_dir, save_name)
                gen_img.save(gen_save_path)
                gt_img.save(gt_save_path)
                
                # 6. Calculate Metrics (Pair-wise)
                # è®¡ç®—æˆå¯¹æŒ‡æ ‡ (éœ€è¦è½¬ä¸º Tensor å¹¶å½’ä¸€åŒ–)
                gen_tensor = read_image(gen_save_path).float() / 255.0
                gt_tensor = read_image(gt_save_path).float() / 255.0
                
                # Add batch dimension
                gen_tensor = gen_tensor.unsqueeze(0).to(device)
                gt_tensor = gt_tensor.unsqueeze(0).to(device)
                
                # Calculate SSIM, PSNR
                scores["ssim"].append(ssim_calc(gen_tensor, gt_tensor).item())
                scores["psnr"].append(psnr_calc(gen_tensor, gt_tensor).item())
                
                # Calculate LPIPS (Requires input in range [-1, 1])
                # LPIPS éœ€è¦è¾“å…¥èŒƒå›´åœ¨ [-1, 1]
                scores["lpips"].append(lpips_calc(gen_tensor * 2.0 - 1.0, gt_tensor * 2.0 - 1.0).item())
                
                # Update FID (Distribution-wise)
                # æ›´æ–° FID ç»Ÿè®¡æ•°æ® (éœ€è¦ uint8 æ ¼å¼)
                gen_uint8 = read_image(gen_save_path).unsqueeze(0).to(device)
                gt_uint8 = read_image(gt_save_path).unsqueeze(0).to(device)
                fid_calc.update(gt_uint8, real=True)
                fid_calc.update(gen_uint8, real=False)
                
            except Exception as e:
                print(f"âš ï¸ Error processing {sat_name}: {e}")
                continue

        # Compute Final FID for this category
        # è®¡ç®—è¯¥ç±»åˆ«çš„ FID åˆ†æ•°
        try:
            fid_score = fid_calc.compute().item()
        except Exception:
            print("âš ï¸ Not enough samples for FID, setting to NaN")
            fid_score = float('nan')

        # Store Results
        # ä¿å­˜è¯¥ç±»åˆ«çš„å¹³å‡ç»“æœ
        final_results.append({
            "Category": severity,
            "SSIM â†‘": np.mean(scores["ssim"]),
            "PSNR â†‘": np.mean(scores["psnr"]),
            "LPIPS â†“": np.mean(scores["lpips"]),
            "FID â†“": fid_score
        })

    # ==========================================
    # 5. Final Report (ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š)
    # ==========================================
    if not final_results:
        print("âŒ No results generated. Check your paths.")
        return

    df_res = pd.DataFrame(final_results)
    
    # Calculate Average across all categories
    # è®¡ç®—æ‰€æœ‰ç±»åˆ«çš„å¹³å‡å€¼
    avg_row = df_res.mean(numeric_only=True).to_dict()
    avg_row["Category"] = "AVERAGE"
    df_res = pd.concat([df_res, pd.DataFrame([avg_row])], ignore_index=True)
    
    # Format columns
    df_res = df_res[["Category", "SSIM â†‘", "PSNR â†‘", "FID â†“", "LPIPS â†“"]]

    print("\n" + "="*60)
    print("ğŸ† FINAL EVALUATION METRICS (æœ€ç»ˆè¯„ä¼°ç»“æœ)")
    print("="*60)
    print(df_res.round(4).to_string(index=False))
    print("="*60)
    
    # Save CSV to D drive
    # ä¿å­˜ CSV æ–‡ä»¶
    save_csv = os.path.join(EVAL_ROOT, "final_metrics.csv")
    df_res.to_csv(save_csv, index=False)
    print(f"\nâœ… All results saved to: {EVAL_ROOT}")

if __name__ == "__main__":
    evaluate_and_calculate()
