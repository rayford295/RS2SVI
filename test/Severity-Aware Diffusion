import torch
import torch.nn as nn
from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler, ControlNetModel, UniPCMultistepScheduler
from transformers import CLIPTextModel, CLIPTokenizer
from PIL import Image
import numpy as np

# è®¾ç½®è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ==========================================
# å¤åˆ¶ä½ çš„æ¨¡å‹ç±»å®šä¹‰
# ==========================================
class SeverityAwareDiffusion(nn.Module):
    def __init__(self, base_model_path):
        super().__init__()
        # è¿™é‡Œä¸éœ€è¦é‡æ–°åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼Œæˆ‘ä»¬ç¨åä¼šè¦†ç›–å®ƒä»¬
        # ä¸ºäº†åˆå§‹åŒ–ç»“æ„ï¼Œæˆ‘ä»¬éœ€è¦å…ˆå®ä¾‹åŒ–ï¼Œä½†å¯ä»¥å…ˆåŠ è½½ç©ºçš„æˆ–è€…æ ‡å‡†çš„
        self.text_encoder = CLIPTextModel.from_pretrained(base_model_path, subfolder="text_encoder")
        self.vae = AutoencoderKL.from_pretrained(base_model_path, subfolder="vae")
        self.unet = UNet2DConditionModel.from_pretrained(base_model_path, subfolder="unet")
        self.controlnet = ControlNetModel.from_unet(self.unet)
        
        self.severity_embedding = nn.Embedding(num_embeddings=3, embedding_dim=768)
        
    def forward(self, pixel_values, conditioning_pixel_values, input_ids, severity_idx, noise, timesteps):
        # æ¨ç†æ—¶è¿™ä¸ª forward å…¶å®ç”¨ä¸åˆ°ï¼Œæˆ‘ä»¬ä¼šæ‰‹å†™æ¨ç†å¾ªç¯
        # ä½†ä¸ºäº†åŠ è½½æƒé‡ï¼Œè¿™ä¸ªç»“æ„å¿…é¡»å­˜åœ¨
        pass

# ==========================================
# é…ç½®è·¯å¾„ (æ ¹æ®ä½ æˆªå›¾ä¸­çš„å®é™…è·¯å¾„ä¿®æ”¹)
# ==========================================
BASE_MODEL_ID = "runwayml/stable-diffusion-v1-5"

# ğŸ” ä¿®æ”¹å¤„ï¼šæ ¹æ®ä½ çš„æˆªå›¾ï¼Œè·¯å¾„åº”æŒ‡å‘ sat2street_model é‡Œçš„ output_severity_diffusion
CHECKPOINT_PATH = "/content/drive/MyDrive/sat2street_model/output_severity_diffusion/checkpoint_epoch_5" 

print(f"ğŸ“‚ æ¨¡å‹è·¯å¾„è®¾ç½®ä¸º: {CHECKPOINT_PATH}")

print("â³ æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹å’Œ tokenizer...")
tokenizer = CLIPTokenizer.from_pretrained(BASE_MODEL_ID, subfolder="tokenizer")
scheduler = UniPCMultistepScheduler.from_pretrained(BASE_MODEL_ID, subfolder="scheduler") 

print("â³ æ­£åœ¨æ„å»ºæ¨¡å‹ç»“æ„...")
# åˆå§‹åŒ–æ¨¡å‹ç»“æ„
model = SeverityAwareDiffusion(BASE_MODEL_ID).to(device)

print("â³ æ­£åœ¨åŠ è½½è®­ç»ƒå¥½çš„æƒé‡...")
# 1. åŠ è½½ UNet
model.unet = UNet2DConditionModel.from_pretrained(f"{CHECKPOINT_PATH}/unet").to(device)
# 2. åŠ è½½ ControlNet
model.controlnet = ControlNetModel.from_pretrained(f"{CHECKPOINT_PATH}/controlnet").to(device)
# 3. åŠ è½½ Severity Embedding
model.severity_embedding.load_state_dict(torch.load(f"{CHECKPOINT_PATH}/severity_embedding.pth", map_location=device))

model.eval()
print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")

# ==========================================
# æ¨ç†å‡½æ•° (Inference Loop)
# ==========================================
@torch.no_grad()
def generate_image(sat_image_path, severity_idx, prompt, steps=30, seed=42):
    # 1. é¢„å¤„ç†å«æ˜Ÿå›¾ (Control Image)
    sat_img = Image.open(sat_image_path).convert("RGB").resize((512, 512))
    # è½¬æ¢ä¸º Tensor: [1, 3, 512, 512] èŒƒå›´ [-1, 1] (ControlNet éœ€è¦)
    cond_image = torch.from_numpy(np.array(sat_img)).float() / 127.5 - 1.0
    cond_image = cond_image.permute(2, 0, 1).unsqueeze(0).to(device)

    # 2. ç¼–ç  Prompt
    text_inputs = tokenizer(
        prompt, padding="max_length", max_length=tokenizer.model_max_length, truncation=True, return_tensors="pt"
    )
    text_input_ids = text_inputs.input_ids.to(device)
    encoder_hidden_states = model.text_encoder(text_input_ids)[0] # [1, 77, 768]

    # 3. æ³¨å…¥ Severity Embedding (å…³é”®æ­¥éª¤ï¼)
    sev_idx_tensor = torch.tensor([severity_idx], device=device, dtype=torch.long)
    sev_embeds = model.severity_embedding(sev_idx_tensor).unsqueeze(1) # [1, 1, 768]
    # æ‹¼æ¥åˆ° text embedding å‰é¢ (ä¸ä½ è®­ç»ƒæ—¶ä¸€è‡´)
    encoder_hidden_states = torch.cat([sev_embeds, encoder_hidden_states], dim=1) # [1, 78, 768]

    # 4. åˆå§‹åŒ– Latents (éšæœºå™ªå£°)
    generator = torch.Generator(device=device).manual_seed(seed)
    latents = torch.randn(
        (1, model.unet.config.in_channels, 512 // 8, 512 // 8),
        generator=generator,
        device=device,
        dtype=model.unet.dtype
    )
    latents = latents * scheduler.init_noise_sigma

    # 5. è®¾ç½® Scheduler
    scheduler.set_timesteps(steps)
    
    print("ğŸ¨ å¼€å§‹ç”Ÿæˆ...")
    for t in scheduler.timesteps:
        # æ‰©å±• latents ä»¥é€‚åº” ControlNet è¾“å…¥ (å¦‚æœä½¿ç”¨äº† classifier-free guidance éœ€è¦å¤åˆ¶ï¼Œè¿™é‡Œç®€åŒ–æ²¡ç”¨ guidance)
        latent_model_input = scheduler.scale_model_input(latents, t)

        # ControlNet å‰å‘ä¼ æ’­
        down_block_res_samples, mid_block_res_sample = model.controlnet(
            sample=latent_model_input,
            timestep=t,
            encoder_hidden_states=encoder_hidden_states,
            controlnet_cond=cond_image,
            return_dict=False,
        )

        # UNet å‰å‘ä¼ æ’­
        noise_pred = model.unet(
            sample=latent_model_input,
            timestep=t,
            encoder_hidden_states=encoder_hidden_states,
            down_block_additional_residuals=down_block_res_samples,
            mid_block_additional_residual=mid_block_res_sample,
        ).sample

        # Scheduler Step (å»å™ª)
        latents = scheduler.step(noise_pred, t, latents).prev_sample

    # 6. è§£ç  Latents -> Image
    image = model.vae.decode(latents / model.vae.config.scaling_factor, return_dict=False)[0]
    image = (image / 2 + 0.5).clamp(0, 1)
    image = image.cpu().permute(0, 2, 3, 1).float().numpy()
    final_image = Image.fromarray((image[0] * 255).astype(np.uint8))
    
    return final_image, sat_img

# æµ‹è¯•å‚æ•°
TEST_IMG_PATH = "/content/drive/MyDrive/sat2street_model/test dataset/004120_sat.png" # ä½ çš„æµ‹è¯•å«æ˜Ÿå›¾è·¯å¾„
SEVERITY_LEVEL = 2  # 0: Minor, 1: Moderate, 2: Severe
PROMPT = "street view photo, SevereDamage, realistic, 4k"

# ç”Ÿæˆ
result_img, input_sat = generate_image(TEST_IMG_PATH, SEVERITY_LEVEL, PROMPT)

# æ˜¾ç¤ºå¯¹æ¯”
import matplotlib.pyplot as plt

fig, axs = plt.subplots(1, 2, figsize=(12, 6))
axs[0].imshow(input_sat)
axs[0].set_title("Input Satellite")
axs[0].axis("off")

axs[1].imshow(result_img)
axs[1].set_title(f"Generated (Severity {SEVERITY_LEVEL})")
axs[1].axis("off")

plt.show()
