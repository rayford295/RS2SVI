import os
import pandas as pd
import numpy as np
from PIL import Image
from torch.utils.data import Dataset, DataLoader
import torch
import torch.nn as nn
import torch.optim as optim
from tqdm.auto import tqdm
from torchvision.utils import save_image

# ==========================================
# 1. æ ¸å¿ƒé…ç½® (Config)
# ==========================================
CONFIG = {
    "epoch": 0,
    "n_epochs": 100,           # è®­ç»ƒè½®æ•°
    "batch_size": 4,           # æ˜¾å­˜å¤Ÿå¤§å¯æ”¹ 8
    "lr": 0.0002,
    "b1": 0.5,
    "b2": 0.999,
    "img_size": 256,           # Pix2Pix ç»å…¸å°ºå¯¸
    "val_ratio": 0.1,          # éªŒè¯é›†æ¯”ä¾‹
    "lambda_pixel": 100,       # L1 Loss æƒé‡
    "output_dir": "./output_pix2pix",
    "root_dir": "./"           # ä½ çš„æ•°æ®æ ¹ç›®å½• (pairs.csv æ‰€åœ¨ä½ç½®)
}

# åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
os.makedirs(CONFIG["output_dir"], exist_ok=True)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ==========================================
# 2. æ•°æ®é›†ç±» (å†…ç½®åœ¨åŒä¸€ä¸ªä»£ç å—é‡Œ)
# ==========================================
class Sat2StreetDataset(Dataset):
    def __init__(self, root_dir="./", resolution=256, split="train", val_ratio=0.1, model_type="gan"):
        self.resolution = resolution
        self.root_dir = root_dir
        self.model_type = model_type
        
        # 1. å¯»æ‰¾ csv
        csv_path = os.path.join(root_dir, "pairs.csv")
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ° {csv_path}ï¼è¯·ç¡®è®¤æ–‡ä»¶è·¯å¾„ã€‚")

        # 2. è¯»å–å¹¶æ‰“ä¹±æ•°æ®
        df_all = pd.read_csv(csv_path)
        
        # ğŸ”¥ æ ¸å¿ƒï¼šå›ºå®šéšæœºç§å­ 42ï¼Œç¡®ä¿ä»¥åè·‘ Diffusion å¯¹æ¯”æ—¶æµ‹è¯•é›†å®Œå…¨ä¸€æ · ğŸ”¥
        df_all = df_all.sample(frac=1, random_state=42).reset_index(drop=True)
        
        # 3. åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        val_size = int(len(df_all) * val_ratio)
        train_size = len(df_all) - val_size
        
        if split == "train":
            self.df = df_all.iloc[:train_size]
        else:
            self.df = df_all.iloc[train_size:]
            
        print(f"âœ… [{split.upper()}] Dataset Load: {len(self.df)} images.")

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        
        sat_name = os.path.basename(row["sat_path"])
        svi_name = os.path.basename(row["svi_path"])
        sat_path = os.path.join(self.root_dir, "images", sat_name)
        svi_path = os.path.join(self.root_dir, "images", svi_name)

        try:
            # Resize
            sat = Image.open(sat_path).convert("RGB").resize((self.resolution, self.resolution), Image.BICUBIC)
            svi = Image.open(svi_path).convert("RGB").resize((self.resolution, self.resolution), Image.BICUBIC)
        except Exception as e:
            print(f"âš ï¸ Error loading {sat_path}: {e}")
            sat = Image.new('RGB', (self.resolution, self.resolution))
            svi = Image.new('RGB', (self.resolution, self.resolution))

        # å½’ä¸€åŒ–åˆ° [-1, 1]
        sat_t = torch.from_numpy(np.array(sat).astype(np.float32)).permute(2, 0, 1) / 127.5 - 1.0
        svi_t = torch.from_numpy(np.array(svi).astype(np.float32)).permute(2, 0, 1) / 127.5 - 1.0

        return {"A": sat_t, "B": svi_t} # A=Satellite, B=StreetView

# ==========================================
# 3. æ¨¡å‹å®šä¹‰ (Generator & Discriminator)
# ==========================================
def weights_init_normal(m):
    classname = m.__class__.__name__
    if classname.find("Conv") != -1:
        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find("BatchNorm2d") != -1:
        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)
        torch.nn.init.constant_(m.bias.data, 0.0)

class UNetDown(nn.Module):
    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):
        super(UNetDown, self).__init__()
        layers = [nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False)]
        if normalize:
            layers.append(nn.InstanceNorm2d(out_size))
        layers.append(nn.LeakyReLU(0.2))
        if dropout:
            layers.append(nn.Dropout(dropout))
        self.model = nn.Sequential(*layers)
    def forward(self, x): return self.model(x)

class UNetUp(nn.Module):
    def __init__(self, in_size, out_size, dropout=0.0):
        super(UNetUp, self).__init__()
        layers = [
            nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False),
            nn.InstanceNorm2d(out_size),
            nn.ReLU(inplace=True),
        ]
        if dropout: layers.append(nn.Dropout(dropout))
        self.model = nn.Sequential(*layers)
    def forward(self, x, skip_input):
        x = self.model(x)
        x = torch.cat((x, skip_input), 1)
        return x

class GeneratorUNet(nn.Module):
    def __init__(self, in_channels=3, out_channels=3):
        super(GeneratorUNet, self).__init__()
        self.down1 = UNetDown(in_channels, 64, normalize=False)
        self.down2 = UNetDown(64, 128)
        self.down3 = UNetDown(128, 256)
        self.down4 = UNetDown(256, 512, dropout=0.5)
        self.down5 = UNetDown(512, 512, dropout=0.5)
        self.down6 = UNetDown(512, 512, dropout=0.5)
        self.down7 = UNetDown(512, 512, dropout=0.5)
        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)
        self.up1 = UNetUp(512, 512, dropout=0.5)
        self.up2 = UNetUp(1024, 512, dropout=0.5)
        self.up3 = UNetUp(1024, 512, dropout=0.5)
        self.up4 = UNetUp(1024, 512, dropout=0.5)
        self.up5 = UNetUp(1024, 256)
        self.up6 = UNetUp(512, 128)
        self.up7 = UNetUp(256, 64)
        self.final = nn.Sequential(
            nn.Upsample(scale_factor=2),
            nn.ZeroPad2d((1, 0, 1, 0)),
            nn.Conv2d(128, out_channels, 4, padding=1),
            nn.Tanh(),
        )
    def forward(self, x):
        d1 = self.down1(x); d2 = self.down2(d1); d3 = self.down3(d2); d4 = self.down4(d3)
        d5 = self.down5(d4); d6 = self.down6(d5); d7 = self.down7(d6); d8 = self.down8(d7)
        u1 = self.up1(d8, d7); u2 = self.up2(u1, d6); u3 = self.up3(u2, d5); u4 = self.up4(u3, d4)
        u5 = self.up5(u4, d3); u6 = self.up6(u5, d2); u7 = self.up7(u6, d1)
        return self.final(u7)

class Discriminator(nn.Module):
    def __init__(self, in_channels=3):
        super(Discriminator, self).__init__()
        def discriminator_block(in_filters, out_filters, normalization=True):
            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]
            if normalization: layers.append(nn.InstanceNorm2d(out_filters))
            layers.append(nn.LeakyReLU(0.2, inplace=True))
            return layers
        self.model = nn.Sequential(
            *discriminator_block(in_channels * 2, 64, normalization=False),
            *discriminator_block(64, 128),
            *discriminator_block(128, 256),
            *discriminator_block(256, 512),
            nn.ZeroPad2d((1, 0, 1, 0)),
            nn.Conv2d(512, 1, 4, padding=1, bias=False)
        )
    def forward(self, img_A, img_B):
        img_input = torch.cat((img_A, img_B), 1)
        return self.model(img_input)

# ==========================================
# 4. è®­ç»ƒä¸»å¾ªç¯
# ==========================================
# åˆå§‹åŒ–æ¨¡å‹
print("ğŸš€ åˆå§‹åŒ–æ¨¡å‹...")
generator = GeneratorUNet().to(device)
discriminator = Discriminator().to(device)
generator.apply(weights_init_normal)
discriminator.apply(weights_init_normal)

# ä¼˜åŒ–å™¨ & æŸå¤±
optimizer_G = torch.optim.Adam(generator.parameters(), lr=CONFIG["lr"], betas=(CONFIG["b1"], CONFIG["b2"]))
optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=CONFIG["lr"], betas=(CONFIG["b1"], CONFIG["b2"]))
criterion_GAN = torch.nn.MSELoss()
criterion_pixelwise = torch.nn.L1Loss()

# æ•°æ®åŠ è½½
train_dataset = Sat2StreetDataset(root_dir=CONFIG["root_dir"], resolution=CONFIG["img_size"], split="train", val_ratio=CONFIG["val_ratio"])
val_dataset = Sat2StreetDataset(root_dir=CONFIG["root_dir"], resolution=CONFIG["img_size"], split="val", val_ratio=CONFIG["val_ratio"])
train_loader = DataLoader(train_dataset, batch_size=CONFIG["batch_size"], shuffle=True, num_workers=0)
val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0)

print(f"ğŸ”¥ å¼€å§‹è®­ç»ƒ! ç»“æœå°†ä¿å­˜åœ¨: {CONFIG['output_dir']}")

for epoch in range(CONFIG["n_epochs"]):
    generator.train()
    discriminator.train()
    
    tqdm_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{CONFIG['n_epochs']}")
    
    for i, batch in enumerate(tqdm_bar):
        real_A = batch["A"].to(device)
        real_B = batch["B"].to(device)

        # ------------------
        #  Train Generators
        # ------------------
        optimizer_G.zero_grad()
        fake_B = generator(real_A)
        
        # PatchGAN labels
        patch = (1, real_A.size(2) // 16, real_A.size(3) // 16)
        valid = torch.ones((real_A.size(0), *patch), requires_grad=False).to(device)
        fake = torch.zeros((real_A.size(0), *patch), requires_grad=False).to(device)

        pred_fake = discriminator(fake_B, real_A)
        loss_GAN = criterion_GAN(pred_fake, valid)
        loss_pixel = criterion_pixelwise(fake_B, real_B)
        loss_G = loss_GAN + CONFIG["lambda_pixel"] * loss_pixel
        
        loss_G.backward()
        optimizer_G.step()

        # ---------------------
        #  Train Discriminator
        # ---------------------
        optimizer_D.zero_grad()
        pred_real = discriminator(real_B, real_A)
        loss_real = criterion_GAN(pred_real, valid)
        pred_fake = discriminator(fake_B.detach(), real_A)
        loss_fake = criterion_GAN(pred_fake, fake)
        loss_D = 0.5 * (loss_real + loss_fake)
        
        loss_D.backward()
        optimizer_D.step()

        tqdm_bar.set_postfix(G=loss_G.item(), D=loss_D.item())

    # --- éªŒè¯ä¸ä¿å­˜ (æ¯5è½®) ---
    if (epoch + 1) % 5 == 0:
        generator.eval()
        with torch.no_grad():
            val_batch = next(iter(val_loader))
            val_real_A = val_batch["A"].to(device)
            val_real_B = val_batch["B"].to(device)
            val_fake_B = generator(val_real_A)
            
            # æ‹¼æ¥: Input | Generated | GroundTruth
            img_sample = torch.cat((val_real_A, val_fake_B, val_real_B), -1)
            save_path = os.path.join(CONFIG["output_dir"], f"epoch_{epoch+1}.png")
            save_image(img_sample, save_path, nrow=2, normalize=True)
            print(f"ğŸ–¼ï¸ éªŒè¯å›¾å·²ä¿å­˜: {save_path}")
        
        torch.save(generator.state_dict(), os.path.join(CONFIG["output_dir"], f"generator_epoch_{epoch+1}.pth"))
