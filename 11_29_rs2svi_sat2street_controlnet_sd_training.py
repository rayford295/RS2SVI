# -*- coding: utf-8 -*-
"""11.29_RS2SVI_Sat2Street_ControlNet_SD_Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UvRDGhoguiaW-i-Y0zVmApc02BVlsmx3
"""

from google.colab import drive
drive.mount('/content/drive')

# è¿™é‡Œæ”¹æˆä½ åœ¨ Drive é‡Œçš„çœŸå®è·¯å¾„ï¼ˆé‡ç‚¹ï¼ï¼‰
# æ¯”å¦‚ï¼š/content/drive/MyDrive/Dataset/CVIAN_reorganized
RAW_DATA_ROOT = "/content/drive/MyDrive/Manuscript_Ph.D./3rd_DisasterMind AI agent (organizationï¼‰/Dataset/CVIAN_reorganized"


WORK_DIR = "/content/sat2street_data"
IMG_DIR  = f"{WORK_DIR}/images"
PAIRS_CSV = f"{WORK_DIR}/pairs.csv"

import os
os.makedirs(IMG_DIR, exist_ok=True)

!ls "/content/drive/MyDrive/Manuscript_Ph.D./3rd_DisasterMind AI agent (organizationï¼‰/Dataset"

import os
import shutil
import csv

severities = ["0_MinorDamage", "1_ModerateDamage", "2_SevereDamage"]

rows = []
idx = 0

for sev in severities:
    sev_dir = os.path.join(RAW_DATA_ROOT, sev)
    if not os.path.isdir(sev_dir):
        continue

    for folder in sorted(os.listdir(sev_dir)):
        folder_path = os.path.join(sev_dir, folder)
        if not os.path.isdir(folder_path):
            continue

        files = os.listdir(folder_path)
        sat_files = [f for f in files if "Satellite" in f]
        svi_files = [f for f in files if "SVI" in f]

        if not sat_files or not svi_files:
            continue

        sat_src = os.path.join(folder_path, sat_files[0])
        svi_src = os.path.join(folder_path, svi_files[0])

        sat_dst = os.path.join(IMG_DIR, f"{idx:06d}_sat.png")
        svi_dst = os.path.join(IMG_DIR, f"{idx:06d}_svi.png")

        shutil.copy(sat_src, sat_dst)
        shutil.copy(svi_src, svi_dst)

        rows.append([sat_dst, svi_dst, sev])  # æŠŠ severity ä¹Ÿä¿å­˜ä¸€ä¸‹
        idx += 1

print("Total pairs:", len(rows))

with open(PAIRS_CSV, "w", newline="") as f:
    writer = csv.writer(f)
    writer.writerow(["sat_path", "svi_path", "severity"])
    writer.writerows(rows)

print("Pairs CSV saved at:", PAIRS_CSV)

import shutil
import os
from google.colab import drive

# 1. æŒ‚è½½ Google Drive
print("æ­£åœ¨æŒ‚è½½ Google Drive...")
drive.mount('/content/drive')

# 2. å®šä¹‰æºæ–‡ä»¶å¤¹å’Œå‹ç¼©åŒ…åç§°
source_folder = '/content/sat2street_data'  # ä½ çš„æ•°æ®æ ¹ç›®å½•
output_filename = '/content/sat2street_dataset_backup' # å‹ç¼©åŒ…çš„ä¸´æ—¶åå­— (ä¸å¸¦.zipåç¼€ï¼Œç¨‹åºä¼šè‡ªåŠ¨åŠ )
destination_path = '/content/drive/MyDrive/sat2street_dataset_backup.zip' # æœ€ç»ˆå­˜åˆ°ç½‘ç›˜çš„ä½ç½®

# 3. å¼€å§‹å‹ç¼©
print(f"æ­£åœ¨å‹ç¼©æ–‡ä»¶å¤¹: {source_folder}ï¼Œæ–‡ä»¶è¾ƒå¤šè¯·ç¨å€™...")
shutil.make_archive(output_filename, 'zip', source_folder)
print("å‹ç¼©å®Œæˆï¼")

# 4. ç§»åŠ¨åˆ° Google Drive
print(f"æ­£åœ¨å¤åˆ¶åˆ° Google Drive: {destination_path} ...")
shutil.copy(f"{output_filename}.zip", destination_path)

print(f"âœ… æˆåŠŸï¼æ–‡ä»¶å·²ä¿å­˜åœ¨ä½ çš„ Google Drive æ ¹ç›®å½•ä¸‹ï¼Œæ–‡ä»¶åä¸º: sat2street_dataset_backup.zip")

!pip install -q diffusers transformers accelerate xformers torch torchvision pandas pillow opencv-python

import zipfile
import os

# ä½ çš„å¤‡ä»½æ–‡ä»¶åœ¨ Drive é‡Œçš„è·¯å¾„
zip_path = '/content/drive/MyDrive/sat2street_dataset_backup.zip'
extract_path = '/content/sat2street_data' # è§£å‹ç›®æ ‡ä½ç½®

if not os.path.exists(extract_path):
    print(f"æ­£åœ¨è§£å‹æ•°æ®åˆ° {extract_path} ...")
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_path)
    print("âœ… è§£å‹å®Œæˆï¼")
else:
    print("âœ… æ–‡ä»¶å¤¹å·²å­˜åœ¨ï¼Œæ— éœ€é‡å¤è§£å‹ã€‚")

# 1. è¿è¡Œä½ æˆªå›¾é‡Œçš„é‚£ä¸ª Class å®šä¹‰ä»£ç 

# 2. è¿è¡Œä¸‹é¢è¿™æ®µæµ‹è¯•ä»£ç 
csv_file_path = '/content/sat2street_data/pairs.csv' # ç¡®ä¿è¿™ä¸ªè·¯å¾„å¯¹

try:
    dataset = Sat2StreetDataset(csv_file_path, resolution=512)
    sample = dataset[0] # å°è¯•è¯»å–ç¬¬ä¸€å¼ å›¾

    print(f"âœ… æ•°æ®é›†æµ‹è¯•é€šè¿‡ï¼å…±æ‰¾åˆ° {len(dataset)} ç»„æ•°æ®ã€‚")
    print(f"Street View Shape: {sample['pixel_values'].shape} (é¢„æœŸ: 3, 512, 512)")
    print(f"Satellite Shape: {sample['condition_pixel_values'].shape} (é¢„æœŸ: 3, 512, 512)")
    print(f"Prompt: {sample['input_ids']}")

except Exception as e:
    print("âŒ å‡ºé”™äº†ï¼å¯èƒ½æ˜¯è·¯å¾„ä¸å¯¹ã€‚é”™è¯¯ä¿¡æ¯ï¼š")
    print(e)

import torch
import torch.nn.functional as F
from accelerate import Accelerator
from diffusers import DDPMScheduler, AutoencoderKL, UNet2DConditionModel, ControlNetModel
from transformers import CLIPTextModel, CLIPTokenizer
from torch.utils.data import DataLoader
from tqdm.auto import tqdm
import os

# --- âš™ï¸ é…ç½®å‚æ•° (å¯è°ƒæ•´) ---
# æ˜¾å­˜ä¼˜åŒ–è®¾ç½®ï¼šBatch Size è®¾ä¸º 2 æˆ– 4ã€‚å¦‚æœçˆ†æ˜¾å­˜ï¼Œæ”¹æˆ 1ã€‚
BATCH_SIZE = 4
# æ¢¯åº¦ç´¯ç§¯ï¼šå¦‚æœæ˜¯ 1ï¼Œæ„å‘³ç€æ¯ä¸€æ­¥éƒ½æ›´æ–°å‚æ•°ï¼›å¦‚æœæ˜¯ 4ï¼Œæ„å‘³ç€æ”’ 4 æ­¥æ‰æ›´æ–°ä¸€æ¬¡ï¼ˆå˜ç›¸å¢å¤§ Batch Sizeï¼‰
GRADIENT_ACCUMULATION_STEPS = 1
LEARNING_RATE = 1e-5
NUM_EPOCHS = 5 # å»ºè®®å…ˆè·‘ 5 ä¸ª Epoch çœ‹çœ‹æ•ˆæœï¼Œåç»­å¯ä»¥æ¥ç€ç»ƒ
OUTPUT_DIR = "/content/drive/MyDrive/sat2street_model" # æ¨¡å‹ä¿å­˜åˆ°äº‘ç›˜
MODEL_ID = "runwayml/stable-diffusion-v1-5"

def train():
    # 1. åˆå§‹åŒ– Accelerator (æ··åˆç²¾åº¦è®­ç»ƒ)
    accelerator = Accelerator(
        mixed_precision="fp16",
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS
    )

    # 2. åŠ è½½æ¨¡å‹ç»„ä»¶
    print("æ­£åœ¨åŠ è½½æ¨¡å‹ç»„ä»¶ (é¦–æ¬¡è¿è¡Œå¯èƒ½éœ€è¦ä¸‹è½½ï¼Œè¯·ç¨å€™)...")
    tokenizer = CLIPTokenizer.from_pretrained(MODEL_ID, subfolder="tokenizer")
    noise_scheduler = DDPMScheduler.from_pretrained(MODEL_ID, subfolder="scheduler")
    text_encoder = CLIPTextModel.from_pretrained(MODEL_ID, subfolder="text_encoder")
    vae = AutoencoderKL.from_pretrained(MODEL_ID, subfolder="vae")
    unet = UNet2DConditionModel.from_pretrained(MODEL_ID, subfolder="unet")

    # ğŸ”¥ å…³é”®ï¼šä» UNet åˆå§‹åŒ–ä¸€ä¸ªæ–°çš„ ControlNet
    # å®ƒçš„è¾“å…¥é€šé“ä¼šè‡ªåŠ¨åŒ¹é…ï¼ˆè¿™é‡Œæˆ‘ä»¬ç”¨å«æ˜Ÿå›¾ RGB 3é€šé“ï¼Œæ­£å¥½åŒ¹é…ï¼‰
    controlnet = ControlNetModel.from_unet(unet)

    # 3. å†»ç»“å‚æ•° (åªè®­ç»ƒ ControlNetï¼Œå…¶ä»–ä¸åŠ¨)
    vae.requires_grad_(False)
    unet.requires_grad_(False)
    text_encoder.requires_grad_(False)
    controlnet.train()

    # å¼€å¯ Gradient Checkpointing (çœæ˜¾å­˜ç¥å™¨)
    controlnet.enable_gradient_checkpointing()

    # 4. å‡†å¤‡æ•°æ®åŠ è½½å™¨
    # è‡ªå®šä¹‰ collate_fnï¼šæŠŠæ–‡æœ¬ prompt è½¬æˆ token id
    def collate_fn(examples):
        pixel_values = torch.stack([example["pixel_values"] for example in examples])
        condition_pixel_values = torch.stack([example["condition_pixel_values"] for example in examples])
        prompts = [example["input_ids"] for example in examples] # è¿™é‡Œ dataset è¿”å›çš„æ˜¯å­—ç¬¦ä¸²

        # Tokenize æ–‡æœ¬
        inputs = tokenizer(
            prompts, max_length=77, padding="max_length", truncation=True, return_tensors="pt"
        )
        return {
            "pixel_values": pixel_values,
            "condition_pixel_values": condition_pixel_values,
            "input_ids": inputs.input_ids
        }

    # ä½¿ç”¨ä½ ä¹‹å‰å®šä¹‰çš„ dataset
    train_dataloader = DataLoader(
        dataset, # è¿™é‡Œç›´æ¥ç”¨ä½ ä¸Šä¸€æ®µä»£ç ç”Ÿæˆçš„ dataset å˜é‡
        shuffle=True,
        batch_size=BATCH_SIZE,
        collate_fn=collate_fn,
        num_workers=2
    )

    # 5. å‡†å¤‡ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(controlnet.parameters(), lr=LEARNING_RATE)

    # æŠŠç»„ä»¶äº¤ç»™ Accelerator ç®¡ç†
    controlnet, optimizer, train_dataloader = accelerator.prepare(
        controlnet, optimizer, train_dataloader
    )

    # å°†ä¸éœ€è¦è®­ç»ƒçš„æ¨¡å‹ç§»åŠ¨åˆ° GPU å¹¶è½¬ä¸º fp16
    vae.to(accelerator.device, dtype=torch.float16)
    unet.to(accelerator.device, dtype=torch.float16)
    text_encoder.to(accelerator.device, dtype=torch.float16)

    # 6. ğŸš€ å¼€å§‹è®­ç»ƒå¾ªç¯
    print(f"å¼€å§‹è®­ç»ƒï¼æ€»å…±æœ‰ {len(train_dataloader)} ä¸ª Batchã€‚")
    global_step = 0

    for epoch in range(NUM_EPOCHS):
        print(f"\nEpoch {epoch+1}/{NUM_EPOCHS} æ­£åœ¨è¿›è¡Œ...")
        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)

        for step, batch in enumerate(train_dataloader):
            with accelerator.accumulate(controlnet):
                # --- A. å‰å‘è¿‡ç¨‹ ---
                # 1. æŠŠè¡—æ™¯å›¾ (Target) ç¼–ç æˆ Latents
                latents = vae.encode(batch["pixel_values"].to(dtype=torch.float16)).latent_dist.sample()
                latents = latents * 0.18215

                # 2. åŠ å™ª
                noise = torch.randn_like(latents)
                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=latents.device).long()
                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)

                # 3. è·å–æ–‡æœ¬ç‰¹å¾
                encoder_hidden_states = text_encoder(batch["input_ids"])[0]

                # 4. ControlNet æ¨ç† (è¾“å…¥å«æ˜Ÿå›¾ condition)
                down_block_res_samples, mid_block_res_sample = controlnet(
                    noisy_latents,
                    timesteps,
                    encoder_hidden_states=encoder_hidden_states,
                    controlnet_cond=batch["condition_pixel_values"].to(dtype=torch.float16),
                    return_dict=False,
                )

                # 5. UNet æ¨ç† (ç»“åˆ ControlNet çš„ç»“æœ)
                noise_pred = unet(
                    noisy_latents,
                    timesteps,
                    encoder_hidden_states=encoder_hidden_states,
                    down_block_additional_residuals=[sample.to(dtype=torch.float16) for sample in down_block_res_samples],
                    mid_block_additional_residual=mid_block_res_sample.to(dtype=torch.float16),
                ).sample

                # --- B. è®¡ç®— Loss å¹¶åå‘ä¼ æ’­ ---
                loss = F.mse_loss(noise_pred.float(), noise.float(), reduction="mean")
                accelerator.backward(loss)
                optimizer.step()
                optimizer.zero_grad()

            progress_bar.update(1)
            progress_bar.set_postfix(loss=loss.item())
            global_step += 1

        # --- C. ä¿å­˜æ¨¡å‹ (æ¯ä¸ª Epoch ç»“æŸå­˜ä¸€æ¬¡) ---
        save_path = os.path.join(OUTPUT_DIR, f"checkpoint-epoch-{epoch+1}")
        print(f"æ­£åœ¨ä¿å­˜æ¨¡å‹åˆ°: {save_path}")
        # Unwrap model ç”¨äºä¿å­˜åŸå§‹æƒé‡
        unwrapped_model = accelerator.unwrap_model(controlnet)
        unwrapped_model.save_pretrained(save_path)

    print("ğŸ‰ è®­ç»ƒå…¨éƒ¨å®Œæˆï¼")

# è¿è¡Œä¸»å‡½æ•°
train()

import torch
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
from PIL import Image
import matplotlib.pyplot as plt
import os
import random

# ==========================================
# 1. è®¾ç½®è·¯å¾„
# ==========================================
# æˆ‘ä»¬åŠ è½½æœ€åä¸€ä¸ª Epoch çš„æ¨¡å‹ï¼Œç†è®ºä¸Šæ•ˆæœæœ€å¥½
CHECKPOINT_PATH = "/content/drive/MyDrive/sat2street_model/checkpoint-epoch-5"
# å›¾ç‰‡æ•°æ®è·¯å¾„
IMAGE_DIR = "/content/sat2street_data/images"

def test_model():
    print(f"ğŸš€ æ­£åœ¨åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹: {CHECKPOINT_PATH} ...")

    # 1. åŠ è½½ç»„ä»¶
    # æ³¨æ„ï¼šè¿™é‡ŒåŠ è½½çš„æ˜¯ä½ è®­ç»ƒçš„ ControlNet éƒ¨åˆ†
    controlnet = ControlNetModel.from_pretrained(CHECKPOINT_PATH, torch_dtype=torch.float16)

    # åŠ è½½åº•æ¨¡ (SD v1.5) å¹¶æŠŠä½ çš„ ControlNet æ’è¿›å»
    pipe = StableDiffusionControlNetPipeline.from_pretrained(
        "runwayml/stable-diffusion-v1-5",
        controlnet=controlnet,
        torch_dtype=torch.float16,
        safety_checker=None
    ).to("cuda")

    # ä½¿ç”¨ UniPC é‡‡æ ·å™¨åŠ é€Ÿæ¨ç† (ç”Ÿæˆæ›´å¿«)
    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
    pipe.enable_model_cpu_offload() # èŠ‚çœæ˜¾å­˜

    # 2. éšæœºæ‰¾ä¸€å¼ å«æ˜Ÿå›¾æµ‹è¯•
    all_files = os.listdir(IMAGE_DIR)
    sat_files = [f for f in all_files if "sat" in f] # ç­›é€‰å‡ºå«æ˜Ÿå›¾

    if not sat_files:
        print("âŒ æ‰¾ä¸åˆ°æµ‹è¯•å›¾ç‰‡ï¼")
        return

    # éšæœºé€‰ä¸€å¼ 
    test_file = random.choice(sat_files)
    sat_path = os.path.join(IMAGE_DIR, test_file)

    # è¯»å–å›¾ç‰‡
    sat_image = Image.open(sat_path).convert("RGB").resize((512, 512))

    # 3. å¼€å§‹ç”Ÿæˆ
    print(f"ğŸ¨ æ­£åœ¨ç”Ÿæˆå¯¹åº”è¡—æ™¯ (è¾“å…¥: {test_file})...")

    # æç¤ºè¯ (Prompt): æè¿°ä½ æƒ³è¦çš„æ ·å­
    prompt = "street view photography, realistic, ground level view, high quality, 4k, urban area, clear sky"
    # è´Ÿé¢æç¤ºè¯ (Negative Prompt): ä½ ä¸æƒ³è¦çš„æ ·å­
    n_prompt = "low quality, blurry, distorted, satellite view, aerial view, map, watermark"

    output_image = pipe(
        prompt,
        negative_prompt=n_prompt,
        image=sat_image,
        num_inference_steps=20,     # æ­¥æ•°
        controlnet_conditioning_scale=1.0, # æ§åˆ¶åŠ›åº¦ (1.0 = å¼ºåŠ›æ§åˆ¶)
        guidance_scale=7.5
    ).images[0]

    # 4. å±•ç¤ºç»“æœå¯¹æ¯”
    fig, ax = plt.subplots(1, 2, figsize=(12, 6))

    ax[0].imshow(sat_image)
    ax[0].set_title("Input: Satellite Image")
    ax[0].axis("off")

    ax[1].imshow(output_image)
    ax[1].set_title("Output: Generated Street View")
    ax[1].axis("off")

    plt.show()

# è¿è¡Œæµ‹è¯•
test_model()

!pip install torchmetrics lpips

import torch
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
from PIL import Image
import matplotlib.pyplot as plt
import pandas as pd
import os
import random
import numpy as np

# ==========================================
# Configuration (é…ç½®åŒºåŸŸ)
# ==========================================
# Path to your trained model checkpoint (e.g., epoch 5)
# æ¨¡å‹è·¯å¾„
CHECKPOINT_PATH = "/content/drive/MyDrive/sat2street_model/checkpoint-epoch-5"

# Root directory of the dataset
# æ•°æ®é›†æ ¹ç›®å½•
DATA_ROOT = "/content/sat2street_data"
CSV_PATH = os.path.join(DATA_ROOT, "pairs.csv")
IMAGE_DIR = os.path.join(DATA_ROOT, "images")

# Number of samples per category
# æ¯ä¸ªç±»åˆ«æµ‹è¯•çš„å›¾ç‰‡æ•°é‡
SAMPLES_PER_CATEGORY = 10

# ==========================================
# Load Model (åŠ è½½æ¨¡å‹)
# ==========================================
print(f"ğŸš€ Loading model from: {CHECKPOINT_PATH} ...")

# Load ControlNet
# åŠ è½½ ControlNet ç»„ä»¶
controlnet = ControlNetModel.from_pretrained(CHECKPOINT_PATH, torch_dtype=torch.float16)

# Load Stable Diffusion Pipeline with ControlNet
# åŠ è½½å®Œæ•´çš„ç”Ÿæˆç®¡çº¿
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    controlnet=controlnet,
    torch_dtype=torch.float16,
    safety_checker=None
).to("cuda")

# Enable optimizations for speed and memory
# å¼€å¯åŠ é€Ÿå’Œæ˜¾å­˜ä¼˜åŒ–
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()

# ==========================================
# Main Evaluation Function (ä¸»è¯„ä¼°å‡½æ•°)
# ==========================================
def evaluate_by_category():
    # Read the CSV file
    # è¯»å–æ•°æ®ç´¢å¼•æ–‡ä»¶
    if not os.path.exists(CSV_PATH):
        print(f"âŒ Error: CSV file not found at {CSV_PATH}")
        return

    df = pd.read_csv(CSV_PATH)

    # Get unique severity levels
    # è·å–æ‰€æœ‰å”¯ä¸€çš„ç¾å®³ç­‰çº§æ ‡ç­¾
    severities = df['severity'].unique()
    print(f"ğŸ“Š Found {len(severities)} severity categories: {severities}")

    # Iterate through each severity category
    # éå†æ¯ä¸€ä¸ªç¾å®³ç­‰çº§
    for severity in severities:
        print(f"\n{'='*20} Testing Category: {severity} {'='*20}")

        # Filter data for current severity and sample 10 items
        # ç­›é€‰å½“å‰ç±»åˆ«çš„æ•°æ®ï¼Œå¹¶éšæœºæŠ½å– 10 æ¡
        category_df = df[df['severity'] == severity]

        # Handle case if less than 10 images exist
        # å¦‚æœæ ·æœ¬ä¸å¤Ÿ 10 ä¸ªï¼Œå°±å–å…¨éƒ¨
        n_samples = min(SAMPLES_PER_CATEGORY, len(category_df))
        sampled_rows = category_df.sample(n_samples)

        # Create a large figure for this category
        # ä¸ºå½“å‰ç±»åˆ«åˆ›å»ºä¸€ä¸ªå¤§ç”»å¸ƒ
        fig, axes = plt.subplots(n_samples, 3, figsize=(20, 5 * n_samples))
        plt.subplots_adjust(wspace=0.1, hspace=0.2)

        # Loop through the sampled rows
        # éå†æŠ½å–çš„æ¯ä¸€è¡Œæ•°æ®
        for idx, (_, row) in enumerate(sampled_rows.iterrows()):
            # Construct file paths
            # æ„å»ºæ–‡ä»¶è·¯å¾„ (ä½¿ç”¨ basename é˜²æ­¢è·¯å¾„å‰ç¼€ä¸åŒ¹é…)
            sat_filename = os.path.basename(row["sat_path"])
            svi_filename = os.path.basename(row["svi_path"])

            sat_full_path = os.path.join(IMAGE_DIR, sat_filename)
            svi_full_path = os.path.join(IMAGE_DIR, svi_filename)

            # Load images
            # è¯»å–å›¾ç‰‡
            try:
                sat_img = Image.open(sat_full_path).convert("RGB")
                gt_svi_img = Image.open(svi_full_path).convert("RGB")
            except FileNotFoundError:
                print(f"âš ï¸ Warning: Image not found for {sat_filename}")
                continue

            # Get Original Dimensions from Ground Truth
            # è·å–çœŸå®è¡—æ™¯çš„åŸå§‹å°ºå¯¸ (å®½, é«˜)
            original_width, original_height = gt_svi_img.size

            # Preprocess Satellite Image (Resize to 512x512 for Model)
            # å«æ˜Ÿå›¾ç¼©æ”¾åˆ° 512x512 ä»¥å–‚ç»™æ¨¡å‹
            input_sat = sat_img.resize((512, 512), Image.BICUBIC)

            # Generate Prompt
            # ç”Ÿæˆæç¤ºè¯
            prompt = f"street view photography, realistic, ground level view, {severity.replace('_', ' ').lower()}, high quality, 4k, urban area"
            negative_prompt = "low quality, blurry, satellite view, aerial view, map, watermark, text"

            # Run Inference
            # æ‰§è¡Œæ¨ç†ç”Ÿæˆ
            with torch.inference_mode():
                generated_output = pipe(
                    prompt,
                    negative_prompt=negative_prompt,
                    image=input_sat,
                    num_inference_steps=20,
                    controlnet_conditioning_scale=1.0,
                    guidance_scale=7.5
                ).images[0]

            # Post-process: Resize Generated Image to Match Original Ground Truth Size
            # åå¤„ç†ï¼šå¼ºåˆ¶å°†ç”Ÿæˆçš„å›¾ç‰‡ç¼©æ”¾å›åŸå§‹å°ºå¯¸
            final_output = generated_output.resize((original_width, original_height), Image.LANCZOS)

            # Plotting (Display in columns)
            # ç»˜å›¾ï¼šå·¦(å«æ˜Ÿ) - ä¸­(ç”Ÿæˆ) - å³(çœŸå®)

            # Column 1: Input Satellite
            # ç¬¬ä¸€åˆ—ï¼šè¾“å…¥å«æ˜Ÿå›¾
            ax = axes[idx] if n_samples > 1 else axes # Handle single row case
            ax[0].imshow(sat_img) # Show original sat
            ax[0].set_title(f"Input Satellite\n({sat_filename})", fontsize=10)
            ax[0].axis("off")

            # Column 2: Generated Street View (Restored Size)
            # ç¬¬äºŒåˆ—ï¼šç”Ÿæˆçš„è¡—æ™¯ (å·²è¿˜åŸå°ºå¯¸)
            ax[1].imshow(final_output)
            ax[1].set_title(f"Generated (Resized to {original_width}x{original_height})", fontsize=10, color="blue")
            ax[1].axis("off")

            # Column 3: Ground Truth (Original Size)
            # ç¬¬ä¸‰åˆ—ï¼šçœŸå®è¡—æ™¯ (åŸå§‹å°ºå¯¸)
            ax[2].imshow(gt_svi_img)
            ax[2].set_title(f"Ground Truth\n({severity})", fontsize=10)
            ax[2].axis("off")

        # Show the plot for this category
        # æ˜¾ç¤ºå½“å‰ç±»åˆ«çš„å›¾åƒç»„
        print(f"âœ… Visualization for {severity} ready:")
        plt.show()

# Run the evaluation
# è¿è¡Œä¸»ç¨‹åº
if __name__ == "__main__":
    evaluate_by_category()

import os
import torch
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
from PIL import Image
import pandas as pd
from tqdm.auto import tqdm

# ================= é…ç½®åŒºåŸŸ =================
# æ¨¡å‹è·¯å¾„
CHECKPOINT_PATH = "/content/drive/MyDrive/sat2street_model/checkpoint-epoch-5"
# æ•°æ®è·¯å¾„
DATA_ROOT = "/content/sat2street_data"
CSV_PATH = os.path.join(DATA_ROOT, "pairs.csv")
IMAGE_DIR = os.path.join(DATA_ROOT, "images")

# ä¿å­˜ç»“æœçš„æ ¹ç›®å½•
RESULTS_DIR = "/content/evaluation_results"
GEN_DIR = os.path.join(RESULTS_DIR, "generated")      # å­˜æ”¾ AI ç”Ÿæˆçš„å›¾
GT_DIR = os.path.join(RESULTS_DIR, "ground_truth")    # å­˜æ”¾ çœŸå®å‚è€ƒå›¾

# æµ‹è¯•æ•°é‡ (è®¾ä¸º None åˆ™æµ‹è¯•å…¨éƒ¨æ•°æ®ï¼Œå»ºè®®å…ˆæµ‹ 50-100 å¼ è¯•è¯•)
NUM_TEST_SAMPLES = 100
# ===========================================

def generate_and_save():
    # 1. åˆ›å»ºæ–‡ä»¶å¤¹
    os.makedirs(GEN_DIR, exist_ok=True)
    os.makedirs(GT_DIR, exist_ok=True)

    # 2. åŠ è½½æ¨¡å‹
    print("ğŸš€ Loading model...")
    controlnet = ControlNetModel.from_pretrained(CHECKPOINT_PATH, torch_dtype=torch.float16)
    pipe = StableDiffusionControlNetPipeline.from_pretrained(
        "runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16, safety_checker=None
    ).to("cuda")
    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
    pipe.enable_model_cpu_offload()

    # 3. è¯»å–æ•°æ®
    df = pd.read_csv(CSV_PATH)
    if NUM_TEST_SAMPLES:
        df = df.sample(min(NUM_TEST_SAMPLES, len(df)), random_state=42) # éšæœºæŠ½æ ·

    print(f"ğŸ“Š Starting generation for {len(df)} images...")

    for idx, row in tqdm(df.iterrows(), total=len(df)):
        try:
            # è·¯å¾„å¤„ç†
            sat_name = os.path.basename(row["sat_path"])
            svi_name = os.path.basename(row["svi_path"])

            sat_path = os.path.join(IMAGE_DIR, sat_name)
            svi_path = os.path.join(IMAGE_DIR, svi_name)

            # è¯»å–å›¾ç‰‡
            sat_img = Image.open(sat_path).convert("RGB")
            gt_img = Image.open(svi_path).convert("RGB")

            # è®°å½•åŸå§‹å°ºå¯¸ç”¨äºè¿˜åŸ
            orig_w, orig_h = gt_img.size

            # ç”Ÿæˆ
            input_sat = sat_img.resize((512, 512), Image.BICUBIC)
            prompt = f"street view photography, realistic, ground level view, {row['severity'].replace('_', ' ').lower()}, high quality, 4k"

            with torch.inference_mode():
                gen_img = pipe(
                    prompt,
                    image=input_sat,
                    num_inference_steps=20,
                    controlnet_conditioning_scale=1.0
                ).images[0]

            # è¿˜åŸå°ºå¯¸
            gen_img = gen_img.resize((orig_w, orig_h), Image.LANCZOS)

            # ä¿å­˜ (ä½¿ç”¨ç›¸åŒçš„ç´¢å¼•åï¼Œæ–¹ä¾¿å¯¹åº”)
            save_name = f"{idx:05d}.png"
            gen_img.save(os.path.join(GEN_DIR, save_name))
            gt_img.save(os.path.join(GT_DIR, save_name))

        except Exception as e:
            print(f"âš ï¸ Error processing {sat_name}: {e}")

    print(f"âœ… All images saved to {RESULTS_DIR}")

if __name__ == "__main__":
    generate_and_save()

import os
import torch
import pandas as pd
import numpy as np
from PIL import Image
from tqdm.auto import tqdm
import matplotlib.pyplot as plt
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler

# Metrics libraries
from torchmetrics.image import StructuralSimilarityIndexMeasure, PeakSignalNoiseRatio, LearnedPerceptualImagePatchSimilarity
from torchmetrics.image.fid import FrechetInceptionDistance
from torchvision.io import read_image

# ==========================================
# 1. Configuration (é…ç½®åŒºåŸŸ)
# ==========================================
# Paths (è·¯å¾„è®¾ç½®)
CHECKPOINT_PATH = "/content/drive/MyDrive/sat2street_model/checkpoint-epoch-5" # ä½ çš„æ¨¡å‹è·¯å¾„
DATA_ROOT = "/content/sat2street_data"
CSV_PATH = os.path.join(DATA_ROOT, "pairs.csv")
IMAGE_DIR = os.path.join(DATA_ROOT, "images")

# Output directory for evaluation results
# ç»“æœä¿å­˜æ ¹ç›®å½•
EVAL_ROOT = "/content/evaluation_results_by_category"

# Number of samples to test per category (Set to None for all images)
# æ¯ä¸ªç±»åˆ«æµ‹è¯•å¤šå°‘å¼ ï¼Ÿå»ºè®®è®¾ä¸º 50 æˆ– 100 ä»¥è·å¾—å‡†ç¡®çš„ FIDã€‚å¦‚æœè®¾ä¸º None åˆ™æµ‹è¯•å…¨éƒ¨ã€‚
SAMPLES_PER_CATEGORY = 20

# ==========================================
# 2. Model Loading (åŠ è½½æ¨¡å‹)
# ==========================================
print(f"ğŸš€ Loading Model from {CHECKPOINT_PATH}...")
controlnet = ControlNetModel.from_pretrained(CHECKPOINT_PATH, torch_dtype=torch.float16)
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    controlnet=controlnet,
    torch_dtype=torch.float16,
    safety_checker=None
).to("cuda")
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()

# ==========================================
# 3. Generation & Saving Function (ç”Ÿæˆå¹¶ä¿å­˜)
# ==========================================
def generate_and_save_images():
    print("\n" + "="*50)
    print("ğŸ¨ Step 1: Generating and Saving Images by Category...")
    print("="*50)

    df = pd.read_csv(CSV_PATH)
    severities = df['severity'].unique() # è·å–æ‰€æœ‰åˆ†ç±»ï¼Œä¾‹å¦‚ ['0_Minor', '1_Moderate', '2_Severe']

    for severity in severities:
        print(f"\nğŸ“‚ Processing Category: {severity}")

        # 1. Create directories for this category
        # ä¸ºå½“å‰ç±»åˆ«åˆ›å»ºæ–‡ä»¶å¤¹: generated å’Œ ground_truth
        cat_gen_dir = os.path.join(EVAL_ROOT, severity, "generated")
        cat_gt_dir = os.path.join(EVAL_ROOT, severity, "ground_truth")
        os.makedirs(cat_gen_dir, exist_ok=True)
        os.makedirs(cat_gt_dir, exist_ok=True)

        # 2. Filter and sample data
        # ç­›é€‰å¹¶æŠ½æ ·æ•°æ®
        cat_df = df[df['severity'] == severity]
        if SAMPLES_PER_CATEGORY:
            cat_df = cat_df.sample(min(SAMPLES_PER_CATEGORY, len(cat_df)), random_state=42)

        # 3. Loop through images
        # éå†å›¾ç‰‡è¿›è¡Œç”Ÿæˆ
        for idx, row in tqdm(cat_df.iterrows(), total=len(cat_df), desc=f"{severity}"):
            try:
                # Load images
                # è¯»å–å›¾ç‰‡
                sat_path = os.path.join(IMAGE_DIR, os.path.basename(row["sat_path"]))
                svi_path = os.path.join(IMAGE_DIR, os.path.basename(row["svi_path"]))

                sat_img = Image.open(sat_path).convert("RGB")
                gt_img = Image.open(svi_path).convert("RGB") # Ground Truth

                # Get Original Size for restoration
                # è·å–çœŸå®å›¾ç‰‡çš„åŸå§‹å°ºå¯¸ (ä¾‹å¦‚ 1024x512)
                orig_w, orig_h = gt_img.size

                # Resize input for model (512x512)
                # ç¼©æ”¾è¾“å…¥ä»¥é€‚é…æ¨¡å‹
                input_sat = sat_img.resize((512, 512), Image.BICUBIC)

                # Generate
                # ç”Ÿæˆ
                prompt = f"street view photography, realistic, ground level view, {severity.replace('_', ' ').lower()}, high quality, 4k"
                with torch.inference_mode():
                    gen_img = pipe(
                        prompt,
                        image=input_sat,
                        num_inference_steps=20,
                        controlnet_conditioning_scale=1.0
                    ).images[0]

                # Resize Generated Image back to Original Size
                # ğŸ”¥ å…³é”®æ­¥éª¤ï¼šå°†ç”Ÿæˆçš„å›¾ resize å›åŸå§‹å°ºå¯¸ (1024x512)
                gen_img = gen_img.resize((orig_w, orig_h), Image.LANCZOS)

                # Save Images (Use same filename for easy matching)
                # ä¿å­˜å›¾ç‰‡
                save_name = f"{idx}.png"
                gen_img.save(os.path.join(cat_gen_dir, save_name))
                gt_img.save(os.path.join(cat_gt_dir, save_name))

            except Exception as e:
                print(f"âš ï¸ Error: {e}")

# ==========================================
# 4. Metrics Calculation Function (è®¡ç®—æŒ‡æ ‡)
# ==========================================
def calculate_metrics_by_category():
    print("\n" + "="*50)
    print("ğŸ“Š Step 2: Calculating Metrics (SSIM, PSNR, LPIPS, FID)...")
    print("="*50)

    device = "cuda" if torch.cuda.is_available() else "cpu"

    # Initialize Metrics
    # åˆå§‹åŒ–æŒ‡æ ‡è®¡ç®—å™¨
    ssim = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)
    psnr = PeakSignalNoiseRatio(data_range=1.0).to(device)
    lpips = LearnedPerceptualImagePatchSimilarity(net_type='vgg').to(device)
    # Note: FID usually requires 64 or 2048 features.
    # For small batch size, we use a smaller feature dimension to avoid errors if needed, but 2048 is standard.
    fid = FrechetInceptionDistance(feature=2048).to(device)

    # Prepare DataFrame to store results
    # å‡†å¤‡è¡¨æ ¼å­˜å‚¨ç»“æœ
    results_list = []

    # Get all categories from the folder structure
    # ä»æ–‡ä»¶å¤¹ç»“æ„ä¸­è·å–æ‰€æœ‰ç±»åˆ«
    categories = [d for d in os.listdir(EVAL_ROOT) if os.path.isdir(os.path.join(EVAL_ROOT, d))]

    for category in categories:
        print(f"\nğŸ“‰ Calculating metrics for: {category} ...")

        gen_dir = os.path.join(EVAL_ROOT, category, "generated")
        gt_dir = os.path.join(EVAL_ROOT, category, "ground_truth")

        # Get list of files
        files = os.listdir(gen_dir)

        # Temporary lists for per-image metrics
        cat_ssim, cat_psnr, cat_lpips = [], [], []

        # Reset FID for each category
        fid.reset()

        for f in tqdm(files, desc="Processing pairs"):
            gen_path = os.path.join(gen_dir, f)
            gt_path = os.path.join(gt_dir, f)

            # Read images into Tensor [0, 1]
            # è¯»å–å›¾ç‰‡å¹¶è½¬ä¸º Tensor
            gen_tensor = read_image(gen_path).float() / 255.0
            gt_tensor = read_image(gt_path).float() / 255.0

            # Add batch dimension (1, C, H, W)
            gen_tensor = gen_tensor.unsqueeze(0).to(device)
            gt_tensor = gt_tensor.unsqueeze(0).to(device)

            # Calculate Paired Metrics
            # è®¡ç®—æˆå¯¹æŒ‡æ ‡
            cat_ssim.append(ssim(gen_tensor, gt_tensor).item())
            cat_psnr.append(psnr(gen_tensor, gt_tensor).item())
            # LPIPS expects input in range [-1, 1]
            cat_lpips.append(lpips(gen_tensor * 2.0 - 1.0, gt_tensor * 2.0 - 1.0).item())

            # Update FID stats (requires uint8 [0, 255] format)
            # æ›´æ–° FID ç»Ÿè®¡æ•°æ® (éœ€è¦ uint8 æ ¼å¼)
            gen_uint8 = read_image(gen_path).unsqueeze(0).to(device)
            gt_uint8 = read_image(gt_path).unsqueeze(0).to(device)
            fid.update(gt_uint8, real=True)
            fid.update(gen_uint8, real=False)

        # Compute FID for the whole category
        # è®¡ç®—è¯¥ç±»åˆ«çš„ FID
        try:
            fid_score = fid.compute().item()
        except Exception as e:
            print(f"âš ï¸ Warning: FID calculation failed (maybe too few samples). Error: {e}")
            fid_score = float('nan')

        # Store results
        # å­˜å‚¨è¯¥ç±»åˆ«çš„å¹³å‡ç»“æœ
        results_list.append({
            "Method": "Sat2Street (Ours)",
            "Category": category,
            "SSIM â†‘": sum(cat_ssim) / len(cat_ssim),
            "PSNR â†‘": sum(cat_psnr) / len(cat_psnr),
            "FID â†“": fid_score,
            "LPIPS â†“": sum(cat_lpips) / len(cat_lpips)
        })

    # Create Final DataFrame
    # åˆ›å»ºæœ€ç»ˆè¡¨æ ¼
    df_results = pd.DataFrame(results_list)

    # Calculate Average across all categories
    # è®¡ç®—æ‰€æœ‰ç±»åˆ«çš„å¹³å‡å€¼
    avg_row = df_results.iloc[:, 2:].mean().to_dict()
    avg_row["Method"] = "Sat2Street (Ours)"
    avg_row["Category"] = "AVERAGE"

    # Use pd.concat instead of append
    df_results = pd.concat([df_results, pd.DataFrame([avg_row])], ignore_index=True)

    # Display Table
    # æ˜¾ç¤ºè¡¨æ ¼
    print("\n" + "="*60)
    print("ğŸ† FINAL EVALUATION RESULTS (æŒ‰ç±»åˆ«åˆ†ç±»æŒ‡æ ‡)")
    print("="*60)
    print(df_results.to_string(index=False))

    # Save to CSV
    # ä¿å­˜ç»“æœæ–‡ä»¶
    save_path = os.path.join(EVAL_ROOT, "final_metrics.csv")
    df_results.to_csv(save_path, index=False)
    print(f"\nâœ… Results saved to: {save_path}")

# ==========================================
# 5. Run Execution (è¿è¡Œä¸»ç¨‹åº)
# ==========================================
if __name__ == "__main__":
    # Step 1: Generate Images
    generate_and_save_images()

    # Step 2: Calculate Metrics
    calculate_metrics_by_category()