import torch
import torch.nn.functional as F
from accelerate import Accelerator
from diffusers import DDPMScheduler, AutoencoderKL, UNet2DConditionModel, ControlNetModel
from transformers import CLIPTextModel, CLIPTokenizer
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import pandas as pd
import numpy as np
import os
from tqdm.auto import tqdm

# ==========================================
# 0. æ˜¾å¡è‡ªæ£€ (Pre-flight Check)
# ==========================================
print(f"ğŸ” æ­£åœ¨æ£€æŸ¥ç¯å¢ƒ...")
if torch.cuda.is_available():
    print(f"âœ… æ˜¾å¡å·²å°±ç»ª: {torch.cuda.get_device_name(0)}")
    print(f"ğŸš€ PyTorch ç‰ˆæœ¬: {torch.__version__}")
else:
    raise RuntimeError("âŒ è‡´å‘½é”™è¯¯ï¼šä¾ç„¶æ²¡æ£€æµ‹åˆ°æ˜¾å¡ï¼è¯·åŠ¡å¿…å…ˆã€é‡å¯å†…æ ¸ã€‘å†è¿è¡Œï¼")

# ==========================================
# 1. é…ç½®åŒºåŸŸ
# ==========================================
BATCH_SIZE = 6          # 3090 24G æ˜¾å­˜ï¼Œè·‘ 6 å¼ å›¾æ²¡é—®é¢˜
NUM_EPOCHS = 10         # è®­ç»ƒ 10 è½®
LEARNING_RATE = 1e-5    # å­¦ä¹ ç‡
OUTPUT_DIR = "./sat2street_output" # æ¨¡å‹ä¿å­˜è·¯å¾„ (ä¼šä¿å­˜åœ¨å½“å‰æ–‡ä»¶å¤¹ä¸‹)
MODEL_ID = "runwayml/stable-diffusion-v1-5" # åŸºç¡€æ¨¡å‹
VAL_RATIO = 0.1         # 10% çš„æ•°æ®ç•™ä½œæµ‹è¯•é›†

# ==========================================
# 2. æ™ºèƒ½æ•°æ®é›†ç±» (å¸¦è‡ªåŠ¨åˆ’åˆ†åŠŸèƒ½)
# ==========================================
class Sat2StreetDataset(Dataset):
    def __init__(self, root_dir="./", resolution=512, split="train", val_ratio=0.1):
        """
        root_dir: å½“å‰ç›®å½• (å› ä¸ºä½ çš„ notebook å°±åœ¨ data æ–‡ä»¶å¤¹é‡Œ)
        split: 'train' (è®­ç»ƒé›†) æˆ– 'val' (éªŒè¯/æµ‹è¯•é›†)
        """
        self.resolution = resolution
        self.root_dir = root_dir
        
        # 1. å¯»æ‰¾ csv
        csv_path = os.path.join(root_dir, "pairs.csv")
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ° {csv_path}ï¼è¯·ç¡®è®¤æ–‡ä»¶å°±åœ¨æ—è¾¹ã€‚")

        # 2. è¯»å–å¹¶æ‰“ä¹±æ•°æ®
        df_all = pd.read_csv(csv_path)
        # å›ºå®šéšæœºç§å­ 42ï¼Œç¡®ä¿æ¯æ¬¡åˆ’åˆ†éƒ½ä¸€æ ·
        df_all = df_all.sample(frac=1, random_state=42).reset_index(drop=True)
        
        # 3. åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        val_size = int(len(df_all) * val_ratio)
        train_size = len(df_all) - val_size
        
        if split == "train":
            self.df = df_all.iloc[:train_size] # å–å‰ 90%
            print(f"âœ… [{split.upper()}] åŠ è½½æˆåŠŸ: {len(self.df)} å¼ å›¾ç‰‡ (è®­ç»ƒç”¨)")
        else:
            self.df = df_all.iloc[train_size:] # å–å 10%
            print(f"âœ… [{split.upper()}] åŠ è½½æˆåŠŸ: {len(self.df)} å¼ å›¾ç‰‡ (æµ‹è¯•ç”¨)")

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        
        # æ— è®º CSV é‡Œå†™å•¥è·¯å¾„ï¼Œåªå–æ–‡ä»¶å
        sat_name = os.path.basename(row["sat_path"])
        svi_name = os.path.basename(row["svi_path"])
        
        # æ‹¼æ¥æœ¬åœ°çœŸå®è·¯å¾„
        sat_path = os.path.join(self.root_dir, "images", sat_name)
        svi_path = os.path.join(self.root_dir, "images", svi_name)

        # è¯»å–ä¸å¤„ç†
        try:
            sat = Image.open(sat_path).convert("RGB").resize((self.resolution, self.resolution))
            svi = Image.open(svi_path).convert("RGB").resize((self.resolution, self.resolution))
        except FileNotFoundError:
            # å®¹é”™å¤„ç†ï¼šå¦‚æœæ‰¾ä¸åˆ°å›¾ï¼Œæ‰“å°ä¸€ä¸‹
            print(f"âš ï¸ æ‰¾ä¸åˆ°å›¾ç‰‡: {sat_path}")
            # è¿”å›ä¸€ä¸ªé»‘å›¾é˜²æ­¢å´©æºƒ (æˆ–è€…ä½ å¯ä»¥é€‰æ‹©æŠ¥é”™)
            sat = Image.new('RGB', (self.resolution, self.resolution))
            svi = Image.new('RGB', (self.resolution, self.resolution))

        svi_t = torch.from_numpy(np.array(svi).astype(np.float32) / 127.5 - 1.0).permute(2, 0, 1)
        sat_t = torch.from_numpy(np.array(sat).astype(np.float32) / 255.0).permute(2, 0, 1)

        prompt = f"street view photography, realistic, ground level view, {str(row['severity']).replace('_', ' ').lower()}, high quality, 4k"

        return {"pixel_values": svi_t, "condition_pixel_values": sat_t, "input_ids": prompt}

# ==========================================
# 3. è®­ç»ƒä¸»ç¨‹åº
# ==========================================
def train_main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    accelerator = Accelerator(mixed_precision="fp16")
    
    # ---------------------------------------------------------
    # åŠ è½½æ•°æ® (Train / Val)
    # ---------------------------------------------------------
    try:
        # åªè®­ç»ƒ 90% çš„æ•°æ®
        train_dataset = Sat2StreetDataset(split="train", val_ratio=VAL_RATIO)
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        return

    # åŠ è½½æ¨¡å‹
    print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")
    tokenizer = CLIPTokenizer.from_pretrained(MODEL_ID, subfolder="tokenizer")
    noise_scheduler = DDPMScheduler.from_pretrained(MODEL_ID, subfolder="scheduler")
    text_encoder = CLIPTextModel.from_pretrained(MODEL_ID, subfolder="text_encoder")
    vae = AutoencoderKL.from_pretrained(MODEL_ID, subfolder="vae")
    unet = UNet2DConditionModel.from_pretrained(MODEL_ID, subfolder="unet")
    controlnet = ControlNetModel.from_unet(unet)

    # å†»ç»“å‚æ•°
    vae.requires_grad_(False)
    unet.requires_grad_(False)
    text_encoder.requires_grad_(False)
    controlnet.train()

    def collate_fn(examples):
        pixel_values = torch.stack([x["pixel_values"] for x in examples])
        condition = torch.stack([x["condition_pixel_values"] for x in examples])
        prompts = [x["input_ids"] for x in examples]
        inputs = tokenizer(prompts, max_length=77, padding="max_length", truncation=True, return_tensors="pt")
        return {"pixel_values": pixel_values, "condition_pixel_values": condition, "input_ids": inputs.input_ids}

    # ğŸ”¥ Windows å…³é”®è®¾ç½®: num_workers=0 ğŸ”¥
    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=0)
    
    optimizer = torch.optim.AdamW(controlnet.parameters(), lr=LEARNING_RATE)

    controlnet, optimizer, train_dataloader = accelerator.prepare(controlnet, optimizer, train_dataloader)
    
    vae.to(accelerator.device, dtype=torch.float16)
    unet.to(accelerator.device, dtype=torch.float16)
    text_encoder.to(accelerator.device, dtype=torch.float16)

    print("ğŸ”¥ è®­ç»ƒå¼€å§‹ï¼")
    
    for epoch in range(NUM_EPOCHS):
        print(f"\n=== Epoch {epoch+1}/{NUM_EPOCHS} ===")
        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)
        
        for step, batch in enumerate(train_dataloader):
            with accelerator.accumulate(controlnet):
                latents = vae.encode(batch["pixel_values"].to(dtype=torch.float16)).latent_dist.sample() * 0.18215
                noise = torch.randn_like(latents)
                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=latents.device).long()
                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)
                
                encoder_hidden_states = text_encoder(batch["input_ids"])[0]
                
                down, mid = controlnet(
                    noisy_latents, timesteps, encoder_hidden_states=encoder_hidden_states,
                    controlnet_cond=batch["condition_pixel_values"].to(dtype=torch.float16),
                    return_dict=False
                )
                
                noise_pred = unet(
                    noisy_latents, timesteps, encoder_hidden_states=encoder_hidden_states,
                    down_block_additional_residuals=[s.to(dtype=torch.float16) for s in down],
                    mid_block_additional_residual=mid.to(dtype=torch.float16)
                ).sample
                
                loss = F.mse_loss(noise_pred.float(), noise.float(), reduction="mean")
                accelerator.backward(loss)
                optimizer.step()
                optimizer.zero_grad()
                
            progress_bar.update(1)
            progress_bar.set_postfix(loss=loss.item())
            
        save_path = os.path.join(OUTPUT_DIR, f"checkpoint-epoch-{epoch+1}")
        accelerator.unwrap_model(controlnet).save_pretrained(save_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {save_path}")

if __name__ == "__main__":
    train_main()
